{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepview import DeepView\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "# ---------------------------\n",
    "import demo_utils as demo\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib qt seems to be a bit buggy with notebooks, so we execute it multiple times\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR10 and a some models\n",
    "\n",
    "This notebook tests the DeepView framework on different classifiers\n",
    "\n",
    " * ResNet-20 on CIFAR10\n",
    " * DecisionTree on MNIST\n",
    " * RandomForest on MNIST\n",
    " * KNN on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Created PyTorch model:\t ResNet\n",
      " * Dataset:\t\t CIFAR10\n",
      " * Best Test prec:\t 91.78000183105469\n",
      "Created decision tree\n",
      " * Depth:\t\t 10\n",
      " * Dataset:\t\t MNIST\n",
      " * Train score:\t\t 0.9833055091819699\n",
      "Created random forest\n",
      " * No. of Estimators:\t 100\n",
      " * Dataset:\t\t MNIST\n",
      " * Train score:\t\t 1.0\n",
      "Created knn classifier\n",
      " * No. of Neighbors:\t 10\n",
      " * Dataset:\t\t MNIST\n",
      " * Train score:\t\t 0.9855314412910406\n"
     ]
    }
   ],
   "source": [
    "# device will be detected automatically\n",
    "# Set to 'cpu' or 'cuda:0' to set the device manually\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "testset = demo.make_cifar_dataset()\n",
    "torch_model = demo.create_torch_model(device)\n",
    "\n",
    "digits_X, digits_y = demo.make_digit_dataset()\n",
    "decision_tree = demo.create_decision_tree(digits_X, digits_y, max_depth=10)\n",
    "random_forest = demo.create_random_forest(digits_X, digits_y, n_estimators=100)\n",
    "kn_neighbors = demo.create_kn_neighbors(digits_X, digits_y, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    " 1. Create a wrapper funktion like ```pred_wrapper``` which receives a numpy array of samples and returns according class probabilities from the classifier as numpy arrays\n",
    " 2. Initialize DeepView-object and pass the created method to the constructor\n",
    " 3. Run your code and call ```add_samples(samples, labels)``` at any time to add samples to the visualization together with the ground truth labels.\n",
    "    * The ground truth labels will be visualized along with the predicted labels\n",
    "    * The object will keep track of a maximum number of samples specified by ```max_samples``` and it will throw away the oldest samples first\n",
    " 4. Call the ```show``` method to render the plot\n",
    "\n",
    "The following parameters must be specified on initialization:\n",
    "\n",
    "| <p align=\"left\">Variable               | <p align=\"left\">Meaning           |\n",
    "|------------------------|-------------------|\n",
    "| <p align=\"left\">(!)```pred_wrapper```     | <p align=\"left\">Wrapper function allowing DeepView to use your model. Expects a single argument, which should be a batch of samples to classify. Returns (valid / softmaxed) prediction probabilities for this batch of samples. |\n",
    "| <p align=\"left\">(!)```classes```          | <p align=\"left\">Names of all different classes in the data. |\n",
    "| <p align=\"left\">(!)```max_samples```      | <p align=\"left\">The maximum amount of samples that DeepView will keep track of. When more samples are added, the oldest samples are removed from DeepView. |\n",
    "| <p align=\"left\">(!)```batch_size```       | <p align=\"left\">The batch size used for classification |\n",
    "| <p align=\"left\">(!)```data_shape```       | <p align=\"left\">Shape of the input data (complete shape; excluding the batch dimension) |\n",
    "| <p align=\"left\">```resolution```       | <p align=\"left\">x- and y- Resolution of the decision boundary plot. A high resolution will compute significantly longer than a lower resolution, as every point must be classified, default 100. |\n",
    "| <p align=\"left\">```cmap```             | <p align=\"left\">Name of the colormap that should be used in the plots, default 'tab10'. |\n",
    "| <p align=\"left\">```interactive```      | <p align=\"left\">When ```interactive``` is True, this method is non-blocking to allow plot updates. When ```interactive``` is False, this method is blocking to prevent termination of python scripts, default True. |\n",
    "| <p align=\"left\">```title```            | <p align=\"left\">Title of the deepview-plot. |\n",
    "| <p align=\"left\">```data_viz```         | <p align=\"left\">DeepView has a reactive plot, that responds to mouse clicks and shows the according data sample, when it is clicked. You can pass a custom visualization function, if ```data_viz``` is None, DeepView will try to show each sample as an image, if possible. (optional, default None)  |\n",
    "| <p align=\"left\">```mapper```           | <p align=\"left\">An object that maps samples from the data space to 2D space. Normally UMAP is used for this, but you can pass a custom mapper as well. (optional)  |\n",
    "| <p align=\"left\">```inv_mapper```       | <p align=\"left\">An object that maps samples from the 2D space to the data space. Normally ```deepview.embeddings.InvMapper``` is used for this, but you can pass a custom inverse mapper as well. (optional)  |\n",
    "| <p align=\"left\">```kwargs```       | <p align=\"left\">Configuration for the embeddings in case they are not specifically given in mapper and inv_mapper. Defaults to ```deepview.config.py```.  (optional)  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo with Torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax operation to use in pred_wrapper\n",
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "# this is the prediction wrapper, it encapsulates the call to the model\n",
    "# and does all the casting to the appropriate datatypes\n",
    "def pred_wrapper(x):\n",
    "    with torch.no_grad():\n",
    "        x = np.array(x, dtype=np.float32)\n",
    "        tensor = torch.from_numpy(x).to(device)\n",
    "        logits = torch_model(tensor)\n",
    "        probabilities = softmax(logits).cpu().numpy()\n",
    "    return probabilities\n",
    "\n",
    "def visualization(image, point2d, pred, label=None):\n",
    "    f, a = plt.subplots()\n",
    "    a.set_title('Prediction: %d' % pred)\n",
    "    a.imshow(image.reshape(8,8))\n",
    "\n",
    "# the classes in the dataset to be used as labels in the plots\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# --- Deep View Parameters ----\n",
    "batch_size = 512\n",
    "max_samples = 500\n",
    "data_shape = (3, 32, 32)\n",
    "n = 3\n",
    "lam = .64\n",
    "resolution = 100\n",
    "cmap = 'tab10'\n",
    "title = 'ResNet-20 - CIFAR10'\n",
    "\n",
    "deepview = DeepView(pred_wrapper, classes, max_samples, batch_size, \n",
    "                    data_shape, n, lam, resolution, cmap, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance calculation 20.00 %\n",
      "Distance calculation 40.00 %\n",
      "Distance calculation 60.00 %\n",
      "Distance calculation 80.00 %\n",
      "Distance calculation 100.00 %\n",
      "Embedding samples ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/luca/a87b7b03-b48e-49f3-903a-66c8d0de2388/python_projects/virt_envs/tf2_venv/lib/python3.6/site-packages/umap/umap_.py:1503: UserWarning: using precomputed metric; transform will be unavailable for new data and inverse_transform will be unavailable for all data\n",
      "  \"using precomputed metric; transform will be unavailable for new data and inverse_transform \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing decision regions ...\n",
      "Time to calculate visualization for 150 samples: 37.24 sec\n"
     ]
    }
   ],
   "source": [
    "n_samples = 150\n",
    "sample_ids = np.random.choice(len(testset), n_samples)\n",
    "X = np.array([ testset[i][0].numpy() for i in sample_ids ])\n",
    "Y = np.array([ testset[i][1] for i in sample_ids ])\n",
    "\n",
    "t0 = time.time()\n",
    "deepview.add_samples(X, Y)\n",
    "deepview.show()\n",
    "\n",
    "\n",
    "print('Time to calculate visualization for %d samples: %.2f sec' % (n_samples, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add new samples to the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance calculation 20.00 %\n",
      "Distance calculation 40.00 %\n",
      "Distance calculation 60.00 %\n",
      "Distance calculation 80.00 %\n",
      "Distance calculation 100.00 %\n",
      "Embedding samples ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/luca/a87b7b03-b48e-49f3-903a-66c8d0de2388/python_projects/virt_envs/tf2_venv/lib/python3.6/site-packages/umap/umap_.py:1503: UserWarning: using precomputed metric; transform will be unavailable for new data and inverse_transform will be unavailable for all data\n",
      "  \"using precomputed metric; transform will be unavailable for new data and inverse_transform \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing decision regions ...\n",
      "Time to add 200 samples to visualization: 101.02 sec\n"
     ]
    }
   ],
   "source": [
    "n_new = 200\n",
    "\n",
    "sample_ids = np.random.choice(len(testset), n_new)\n",
    "X = np.array([ testset[i][0].numpy() for i in sample_ids ])\n",
    "Y = np.array([ testset[i][1] for i in sample_ids ])\n",
    "\n",
    "t0 = time.time()\n",
    "deepview.add_samples(X, Y)\n",
    "deepview.show()\n",
    "\n",
    "print('Time to add %d samples to visualization: %.2f sec' % (n_new, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example output\n",
    "\n",
    "As the plot is updatable, it is shown in a separate Qt-window. With the CIFAR-data and the model loaded above, the following plot was produced after 200 samples where added:\n",
    "\n",
    "**Hyperparameters:**\n",
    "n = 10\n",
    "lam = 0.2\n",
    "resolution = 100\n",
    "\n",
    "![sample_plot](https://user-images.githubusercontent.com/30961397/72370639-fbab6f00-3702-11ea-98f4-0dc7335777fc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the $\\lambda$-Hyperparameter\n",
    "\n",
    "> The $\\lambda$-Hyperparameter weights the euclidian distance component.\n",
    "> When the visualization doesn't show class-clusters, **try a smaller lambda** to put more emphasis on the discriminative distance component that considers the class.\n",
    "> A smaller $\\lambda$ will pull the datapoints further into their class-clusters.\n",
    "> Therefore, a **too small $\\lambda$** can lead to collapsed clusters that don't represent any structural properties of the datapoints. Of course this behaviour also depends on the data and how well the label corresponds to certain structural properties.\n",
    "\n",
    "Due to separate handling of euclidian and class-discriminative distances, the $\\lambda$ parameter can easily be adjusted. Distances don't need to be recomputed, only the embeddings and therefore also the plot of the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding samples ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/luca/a87b7b03-b48e-49f3-903a-66c8d0de2388/python_projects/virt_envs/tf2_venv/lib/python3.6/site-packages/umap/umap_.py:1503: UserWarning: using precomputed metric; transform will be unavailable for new data and inverse_transform will be unavailable for all data\n",
      "  \"using precomputed metric; transform will be unavailable for new data and inverse_transform \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing decision regions ...\n"
     ]
    }
   ],
   "source": [
    "deepview.set_lambda(.7)\n",
    "deepview.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare performance\n",
    "\n",
    "For this test, DeepView was run on a GPU (GTX 2060 6GB).\n",
    "Adding samples may be a bit more time consuming, then just running DeepView on the desired amount of samples to be visualized. This is because the decision boundaries must be calculated twice with a similar time complexity. However, the step of adding 100 samples to 100 existing samples takes less time then computing it from scratch for 200 samples. This is because distances were already computed for half of the samples and can be reused.\n",
    "\n",
    "| <p align=\"left\">Szenario | Time |\n",
    "| -------- | ---- |\n",
    "| <p align=\"left\">From scratch for 100 samples | 31.20 sec |\n",
    "| <p align=\"left\">Adding 100 samples (100 already added) | 66.89 sec |\n",
    "| <p align=\"left\">From scratch for 200 samples | 71.16 sec |\n",
    "| <p align=\"left\">200 samples when adding 100 samples in two steps | 98.19 sec |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance calculation 20.00 %\n",
      "Distance calculation 40.00 %\n",
      "Distance calculation 60.00 %\n",
      "Distance calculation 80.00 %\n",
      "Distance calculation 100.00 %\n",
      "Embedding samples ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/luca/a87b7b03-b48e-49f3-903a-66c8d0de2388/python_projects/virt_envs/tf2_venv/lib/python3.6/site-packages/umap/umap_.py:1503: UserWarning: using precomputed metric; transform will be unavailable for new data and inverse_transform will be unavailable for all data\n",
      "  \"using precomputed metric; transform will be unavailable for new data and inverse_transform \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing decision regions ...\n",
      "Time to calculate visualization for 200 samples: 46.88 sec\n"
     ]
    }
   ],
   "source": [
    "deepview.reset()\n",
    "\n",
    "n_samples = 200\n",
    "sample_ids = np.random.choice(len(testset), n_samples)\n",
    "X = np.array([ testset[i][0].numpy() for i in sample_ids ])\n",
    "Y = np.array([ testset[i][1] for i in sample_ids ])\n",
    "\n",
    "t0 = time.time()\n",
    "deepview.add_samples(X, Y)\n",
    "deepview.show()\n",
    "\n",
    "print('Time to calculate visualization for %d samples: %.2f sec' % (n_samples, time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepview.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo with RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(image, point2d, pred, label=None):\n",
    "    f, a = plt.subplots()\n",
    "    a.set_title('Prediction: %d' % pred)\n",
    "    a.imshow(image.reshape(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance calculation 20.00 %\n",
      "Distance calculation 40.00 %\n",
      "Distance calculation 60.00 %\n",
      "Distance calculation 80.00 %\n",
      "Distance calculation 100.00 %\n",
      "Embedding samples ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/luca/a87b7b03-b48e-49f3-903a-66c8d0de2388/python_projects/virt_envs/tf2_venv/lib/python3.6/site-packages/umap/umap_.py:1503: UserWarning: using precomputed metric; transform will be unavailable for new data and inverse_transform will be unavailable for all data\n",
      "  \"using precomputed metric; transform will be unavailable for new data and inverse_transform \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing decision regions ...\n",
      "Time to calculate visualization for 50 samples: 9.52 sec\n"
     ]
    }
   ],
   "source": [
    "pred_wrapper = DeepView.create_simple_wrapper(random_forest.predict_proba)\n",
    "\n",
    "# the digit dataset is used, so classes are [0..9]\n",
    "classes = np.arange(10)\n",
    "\n",
    "# --- Deep View Parameters ----\n",
    "batch_size = 64\n",
    "max_samples = 500\n",
    "sample_shape = (64,)\n",
    "n = 10\n",
    "lam = 0.5\n",
    "resolution = 100\n",
    "cmap = 'tab10'\n",
    "title = 'RandomForest - MNIST'\n",
    "\n",
    "# create DeepView object\n",
    "deepview = DeepView(pred_wrapper, classes, max_samples, batch_size, sample_shape, \n",
    "                    n, lam, resolution, cmap, title=title, data_viz=visualization)\n",
    "\n",
    "# add data samples\n",
    "n_samples = 50\n",
    "sample_ids = np.random.choice(len(digits_X), n_samples)\n",
    "X = np.array([ digits_X[i] for i in sample_ids ])\n",
    "Y = np.array([ digits_y[i] for i in sample_ids ])\n",
    "\n",
    "t0 = time.time()\n",
    "deepview.add_samples(X, Y)\n",
    "deepview.show()\n",
    "\n",
    "print('Time to calculate visualization for %d samples: %.2f sec' % (n_samples, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![random_forest](https://user-images.githubusercontent.com/30961397/78502477-a6ab5200-7761-11ea-8be3-e0b4c8e6a966.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepview.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo with DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Deep View Parameters ----\n",
    "batch_size = 256\n",
    "max_samples = 500\n",
    "# the data can also be represented as a vector\n",
    "sample_shape = (64,)\n",
    "n = 10\n",
    "lam = 0.65\n",
    "resolution = 100\n",
    "cmap = 'gist_ncar'\n",
    "\n",
    "# the digit dataset is used, so classes are [0..9]\n",
    "classes = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance calculation 20.00 %\n",
      "Distance calculation 40.00 %\n",
      "Distance calculation 60.00 %\n",
      "Distance calculation 80.00 %\n",
      "Distance calculation 100.00 %\n",
      "Embedding samples ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/luca/a87b7b03-b48e-49f3-903a-66c8d0de2388/python_projects/virt_envs/tf2_venv/lib/python3.6/site-packages/umap/umap_.py:1503: UserWarning: using precomputed metric; transform will be unavailable for new data and inverse_transform will be unavailable for all data\n",
      "  \"using precomputed metric; transform will be unavailable for new data and inverse_transform \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing decision regions ...\n",
      "Time to calculate visualization for 200 samples: 92.71 sec\n"
     ]
    }
   ],
   "source": [
    "pred_wrapper = DeepView.create_simple_wrapper(kn_neighbors.predict_proba)\n",
    "\n",
    "# create DeepView object\n",
    "deepview = DeepView(pred_wrapper, classes, max_samples, batch_size, \n",
    "                    sample_shape, n, lam, resolution, cmap)\n",
    "\n",
    "# add data samples\n",
    "n_samples = 200\n",
    "sample_ids = np.random.choice(len(digits_X), n_samples)\n",
    "X = np.array([ digits_X[i] for i in sample_ids ])\n",
    "Y = np.array([ digits_y[i] for i in sample_ids ])\n",
    "\n",
    "t0 = time.time()\n",
    "deepview.add_samples(X, Y)\n",
    "deepview.show()\n",
    "\n",
    "print('Time to calculate visualization for %d samples: %.2f sec' % (n_samples, time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding samples ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/luca/a87b7b03-b48e-49f3-903a-66c8d0de2388/python_projects/virt_envs/tf2_venv/lib/python3.6/site-packages/umap/umap_.py:1503: UserWarning: using precomputed metric; transform will be unavailable for new data and inverse_transform will be unavailable for all data\n",
      "  \"using precomputed metric; transform will be unavailable for new data and inverse_transform \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing decision regions ...\n"
     ]
    }
   ],
   "source": [
    "deepview.set_lambda(.4)\n",
    "deepview.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepview.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: KNN-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_wrapper = DeepView.create_simple_wrapper(kn_neighbors.predict_proba)\n",
    "\n",
    "# create DeepView object\n",
    "deepview = DeepView(pred_wrapper, classes, max_samples, batch_size, \n",
    "                    sample_shape, n, lam, resolution, cmap)\n",
    "\n",
    "# add data samples\n",
    "n_samples = 200\n",
    "sample_ids = np.random.choice(len(digits_X), n_samples)\n",
    "X = np.array([ digits_X[i] for i in sample_ids ])\n",
    "Y = np.array([ digits_y[i] for i in sample_ids ])\n",
    "\n",
    "t0 = time.time()\n",
    "deepview.add_samples(X, Y)\n",
    "deepview.show()\n",
    "\n",
    "print('Time to calculate visualization for %d samples: %.2f sec' % (n_samples, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![knn](https://user-images.githubusercontent.com/30961397/78502740-dc046f80-7762-11ea-82cf-efc8251539db.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate \n",
    "\n",
    "These evaluations can be run with an initialized instance of DeepView."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of DeepView: DeepView\n",
      "\n",
      "orig labs, knn err: eucl / fish 0.05 / 0.045\n",
      "orig labs, knn err in proj space: eucl / fish 0.1 / 0.05\n",
      "classif labs, knn err: eucl / fish 0.02 / 0.025\n",
      "classif labs, knn acc in proj space: eucl / fish 93.5 / 98.0\n"
     ]
    }
   ],
   "source": [
    "from deepview.evaluate import evaluate_umap\n",
    "\n",
    "print('Evaluation of DeepView: %s\\n' % deepview.title)\n",
    "evaluate_umap(deepview, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Inverse Mapping\n",
    "\n",
    "Evaluation of the inverse mapping (i.e. the mapping from 2D back into sample-space) is done by first, passing some training samples to DeepView. It will classify them with the given model, train the mappers (UMAP and inverse) on them, and embed them into 2D space.\n",
    "A fraction of the embedded samples will be used to train the inverse mapper from ground up. After reconstructing the same set of samples, they will be classified again. The predictions are compared against the prior predictions from deepview and used to calculate the train accuracy.\n",
    "\n",
    "The spare samples are used as testing samples, they were not used during training of the inverse mapper. They are mapped back into sample-space as well, classified and these classification are used to calculate the test accuracy of the inverse mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance calculation 20.00 %\n",
      "Distance calculation 40.00 %\n",
      "Distance calculation 60.00 %\n",
      "Distance calculation 80.00 %\n",
      "Distance calculation 100.00 %\n",
      "Embedding samples ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/luca/a87b7b03-b48e-49f3-903a-66c8d0de2388/python_projects/virt_envs/tf2_venv/lib/python3.6/site-packages/umap/umap_.py:1503: UserWarning: using precomputed metric; transform will be unavailable for new data and inverse_transform will be unavailable for all data\n",
      "  \"using precomputed metric; transform will be unavailable for new data and inverse_transform \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing decision regions ...\n",
      "Inverse-Mapper train accuracy:\t83.10%\n",
      "Inverse-Mapper test accuracy:\t76.11%\n"
     ]
    }
   ],
   "source": [
    "from deepview.evaluate import evaluate_inv_umap\n",
    "\n",
    "# for testing, reset deepview and add some samples\n",
    "# a fraction of these will serve as training set for the evaluation\n",
    "n_samples = 600\n",
    "fraction = 0.7\n",
    "\n",
    "sample_ids = np.random.choice(len(digits_X), n_samples)\n",
    "X = np.array([ testset[i][0].numpy() for i in sample_ids ])\n",
    "Y = np.array([ testset[i][1] for i in sample_ids ])\n",
    "\n",
    "train_acc, test_acc = evaluate_inv_umap(deepview, X, Y, fraction)\n",
    "\n",
    "print('Inverse-Mapper train accuracy:\\t%.2f%%' % train_acc)\n",
    "print('Inverse-Mapper test accuracy:\\t%.2f%%' % test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepview.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.0",
   "language": "python",
   "name": "tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
